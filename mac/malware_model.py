#coding:utf-8
from __future__ import division 
import tensorflow as tf
from utils import ConvolutionalBatchNormalizer

def lazy_property(func):
    attr_name = "_lazy_" + func.__name__
    @property
    def _lazy_property(self):
        if not hasattr(self, attr_name):
            setattr(self, attr_name, func(self))
        return getattr(self, attr_name)
    return _lazy_property


class malwareModel(object):
    def __init__(self,images,labels,learning_rate=0.1,\
                 height=128,width=128,num_classes=9,one_hot=True):
        """
        Args:
            labels：必须是one_hot编码,shape;[batch_size,num_classes]
            images：shape;[batch_size,columns*rows](assuming depth=1)
            height:
            width:
            num_classes:
        """
        self._images = images
        self._height = height
        self._width = width
        self._num_classes = num_classes
        self._labels = labels
        self._logits = None
        self._optimize = None
        self._learning_rate =learning_rate 
        self._accuracy = None
        self._prob = None
        
    @lazy_property
    def prediction(self):
        """
        Return:
            self._logits:shape;[batch_size,num_classes]
        """
        
        ewma = tf.train.ExponentialMovingAverage(decay=0.99)
        
        with tf.variable_scope("input"):
            images = tf.reshape(self._images,shape=[-1,self._height,self._width,1],name="reshape")
            
        with tf.variable_scope("conv1"):
            bn1 = ConvolutionalBatchNormalizer(32,0.001,ewma,True)
            update_assigement = bn1.get_assigner()
            weights = tf.get_variable("weights",shape=[5,5,1,32],\
                                      initializer=tf.truncated_normal_initializer(stddev=0.01))
            bias = tf.get_variable("bias",shape = [32],\
                                   initializer=tf.constant_initializer(0.1))
            batch_normal_output = bn1.normalize(tf.nn.conv2d(images,weights,\
                                            strides=[1,1,1,1],padding='SAME')+bias,True)
            conv1 = tf.nn.relu(batch_normal_output)
            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,update_assigement)
        with tf.variable_scope("pool1"):
            pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')
            
        with tf.variable_scope('conv2'):
            bn2 = ConvolutionalBatchNormalizer(64,0.001,ewma,True)
            update_assigement = bn2.get_assigner()
            weight = tf.get_variable("weight",shape=[5,5,32,64],initializer=tf.truncated_normal_initializer(stddev=0.01));
            bias = tf.get_variable("bias",shape = [64],initializer=tf.constant_initializer(0.1));
            batch_normal_output = bn2.normalize(tf.nn.conv2d(pool1,weight,strides=[1, 1, 1, 1], padding='SAME')+bias,True)
            conv2 = tf.nn.relu(batch_normal_output)
            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,update_assigement)
        
        with tf.variable_scope('pool2'):
            pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')
    
    
    
        with tf.variable_scope('nn1'):
            pool2_flat = tf.reshape(pool2,[-1,32*32*64]);
            weight = tf.get_variable("weights",shape = [32*32*64,1024],\
                                     initializer=tf.truncated_normal_initializer(stddev = 0.01));
            bias = tf.get_variable("bias",shape=[1024],initializer=tf.constant_initializer(0.1));
            dropout_output = tf.nn.dropout(tf.matmul(pool2_flat,weight)+bias,keep_prob=1)
            nn1_output = tf.nn.tanh(dropout_output);
    
        
        with tf.variable_scope("softmax_layer"):
            weights = tf.get_variable("weights",shape=[1024,1]\
                                      ,initializer=tf.truncated_normal_initializer(stddev=0.01))
            bias = tf.get_variable("bias",shape=[1],dtype=tf.float32,initializer=tf.constant_initializer(0.1))
            _prob =  tf.matmul(nn1_output,weights)+bias
            self._logits = tf.nn.sigmoid(_prob)
        return self._logits
    @lazy_property
    def optimize(self):
        loss = -tf.reduce_mean(self._labels*tf.log(self.prediction)\
                               +(1-self._labels)*tf.log(1-self.prediction))
        #loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=self._labels,logits=self.prediction)
        
        tf.summary.scalar('loss',loss)
        optimizer = tf.train.AdamOptimizer(self._learning_rate)
        self._optimize = optimizer.minimize(loss)
        return loss,self._optimize
        
#==============================================================================
#     def accuracy(self):
#         from sklearn.metrics import f1_score
#         logits = tf.cast(tf.greater(self._logits,0.5),tf.float32)
#         return f1_score(self._labels,logits)
#         correct_prediction= tf.equal(tf.cast(tf.greater(self._logits,0.5),tf.float32),self._labels)
#         self._accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
#         return self._accuracy
#==============================================================================
#from sklearn.metrics import f1_score
import numpy as np
def accuracy(labels,logits):
    """
    Args:
        labels:[batch_size,1]
        logits:[batch_size,1],probability of 1
    """
    #logits = (np.array(logits)>0.5).astype(int)
    logits = (logits>0.5).astype(int)
    #print logits
    true_black_num = np.sum(labels==0);
    predict_black_num = np.sum(logits==0);
    #print predict_black_num
    perdict_true_black_num = np.sum((labels==0)&(logits==0));
    if predict_black_num != 0:
        P = perdict_true_black_num/predict_black_num;
    else:
        P = 0
    if true_black_num !=0:
        R = perdict_true_black_num/true_black_num;
    else:
        R = 0
    return P,R
# =============================================================================
#     logits = (np.array(logits)>0.5).astype(int)
#     prediction = np.mean(labels==logits)
#     return prediction
# =============================================================================
    
    
    
    
    
    
    
    
    
    
    