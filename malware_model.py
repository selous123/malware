#coding:utf-8
import tensorflow as tf
import tensorlayer as tl

def lazy_property(func):
    attr_name = "_lazy_" + func.__name__
    @property
    def _lazy_property(self):
        if not hasattr(self, attr_name):
            setattr(self, attr_name, func(self))
        return getattr(self, attr_name)
    return _lazy_property


class malwareModel(object):
    def __init__(self,images,labels,learning_rate=0.1,\
                 height=128,width=128,num_classes=9,one_hot=True):
        """
        Args:
            labels：必须是one_hot编码,shape;[batch_size,num_classes]
            images：shape;[batch_size,columns*rows](assuming depth=1)
            height:
            width:
            num_classes:
        """
        self._images = images
        self._height = height
        self._width = width
        self._num_classes = num_classes
        self._labels = labels
        self._logits = None
        self._optimize = None
        self._learning_rate =learning_rate 
        self._accuracy = None

        
    @lazy_property
    def prediction(self):
        """
        Return:
            self._logits:shape;[batch_size,num_classes]
        """
        
        with tf.variable_scope("input"):
            images = tf.reshape(self._images,shape=[-1,self._height,self._width,1],name="reshape")
            
        with tf.variable_scope("conv1"):
            weights = tf.get_variable("weights",shape=[5,5,1,32],\
                                      initializer=tf.truncated_normal_initializer(stddev=0.01))
            bias = tf.get_variable("bias",shape = [32],\
                                   initializer=tf.constant_initializer(0.1))
            conv1 = tf.nn.relu(tf.nn.conv2d(images,weights,\
                                            strides=[1,1,1,1],padding='SAME')+bias,name="relu1")
        with tf.variable_scope("pool1"):
            pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')
            
        with tf.variable_scope('conv2'):
            weight = tf.get_variable("weight",shape=[5,5,32,64],initializer=tf.truncated_normal_initializer(stddev=0.01));
            bias = tf.get_variable("bias",shape = [64],initializer=tf.constant_initializer(0.1));
            conv2 = tf.nn.relu(tf.nn.conv2d(pool1,weight,strides=[1, 1, 1, 1], padding='SAME')+bias)
        
        with tf.variable_scope('pool2'):
            pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')
    
    
    
        with tf.variable_scope('nn1'):
            pool2_flat = tf.reshape(pool2,[-1,32*32*64]);
            weight = tf.get_variable("weights",shape = [32*32*64,1024],\
                                     initializer=tf.truncated_normal_initializer(stddev = 0.01));
            bias = tf.get_variable("bias",shape=[1024],initializer=tf.constant_initializer(0.1));
            nn1_output = tf.nn.tanh(tf.matmul(pool2_flat,weight)+bias);
    
        
        with tf.variable_scope("softmax_layer"):
            weights = tf.get_variable("weights",shape=[1024,9]\
                                      ,initializer=tf.truncated_normal_initializer(stddev=0.01))
            bias = tf.get_variable("bias",shape=[9],dtype=tf.float32,initializer=tf.constant_initializer(0.1))
            _prob =  tf.matmul(nn1_output,weights)+bias
            self._logits = tf.nn.softmax(_prob)
        return self._logits
        
    @lazy_property
    def optimize(self):
        loss = -tf.reduce_mean(self._labels*tf.log(self.prediction))
        tf.summary.scalar('loss',loss)
        optimizer = tf.train.AdamOptimizer(self._learning_rate)
        self._optimize = optimizer.minimize(loss)
        return self._optimize,loss
        
    @lazy_property
    def accuracy(self):
        correct_prediction= tf.equal(tf.argmax(self._logits,1), tf.argmax(self._labels,1))
        self._accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
        return self._accuracy